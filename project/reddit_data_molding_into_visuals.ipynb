{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import urllib.request\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import pandas\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import threading_jobs as tj\n",
    "\n",
    "from bokeh.core.properties import value\n",
    "from bokeh.io import show, output_file, export_svgs\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import dodge\n",
    "from bokeh.io import export_svgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... \n",
      "Warning: 2 possible package resolutions (only showing differing packages):\n",
      "  - anaconda::conda-4.7.12-py37_0\n",
      "  - defaults::conda-4.7.12-py37_0done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: D:\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - selenium\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    selenium-3.141.0           |   py37he774522_0         805 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         805 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  selenium           pkgs/main/win-64::selenium-3.141.0-py37he774522_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "selenium-3.141.0     | 805 KB    |            |   0% \n",
      "selenium-3.141.0     | 805 KB    | 1          |   2% \n",
      "selenium-3.141.0     | 805 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --prefix {sys.prefix} -c bokeh selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\"]\n",
    "subreddits = [\"nyc\", \"losangeles\", \"chicago\", \"houston\", \"phoenix\"]\n",
    "\n",
    "# collection of all data aggregated together\n",
    "reddit_master_collection = pandas.read_csv(\"D:\\social_media_analytics\\\\reddit_content\\collection\\sentiment_data_2010-2014.csv\", usecols=[1,2,3,4,5,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 940000\r"
     ]
    }
   ],
   "source": [
    "# date from epoch\n",
    "def getDateFromEpoch(epoch):\n",
    "    return datetime.datetime.utcfromtimestamp(epoch)\n",
    "\n",
    "\n",
    "# convert all of our dataframe rows to have date (year-month-day) instead of epoch\n",
    "for i, row in reddit_master_collection.iterrows():\n",
    "    date = None\n",
    "    date = getDateFromEpoch(reddit_master_collection.at[i, 'created_utc'])\n",
    "    reddit_master_collection.at[i, 'created_utc'] = date\n",
    "    if i % 20000 == 0:\n",
    "        print('Iteration', i, end='\\r')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a base object incase I accidentally destroy the base object\n",
    "# i don't want to have to wait to process a new dataframe if I accidentally destroy it\n",
    "\n",
    "reddit_master_collection2 = reddit_master_collection\n",
    "# reddit_master_collection = reddit_master_collection2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove excess records\n",
    "\n",
    "starting_datetime = datetime.datetime.strptime('2010-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "reddit_master_collection = reddit_master_collection[reddit_master_collection.created_utc >= starting_datetime]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       created_utc             author  \\\n",
      "943920  2010-01-01            DesCo83   \n",
      "943870  2010-01-01           edman007   \n",
      "943871  2010-01-01                jba   \n",
      "943872  2010-01-01         krugerlive   \n",
      "943890  2010-01-01             coasts   \n",
      "943896  2010-01-01          [deleted]   \n",
      "943898  2010-01-01             coasts   \n",
      "943902  2010-01-01            brianoh   \n",
      "943905  2010-01-01           beerbabe   \n",
      "943906  2010-01-01            DesCo83   \n",
      "943869  2010-01-01      SoManyMinutes   \n",
      "943907  2010-01-01            bmeckel   \n",
      "943909  2010-01-01           middkidd   \n",
      "943910  2010-01-01      norwegianrich   \n",
      "943911  2010-01-01        elsagacious   \n",
      "943913  2010-01-01            brianoh   \n",
      "943914  2010-01-01          zwangaman   \n",
      "943915  2010-01-01           beerbabe   \n",
      "943916  2010-01-01            DesCo83   \n",
      "943917  2010-01-01       mrcassavetes   \n",
      "943918  2010-01-01            brianoh   \n",
      "943908  2010-01-01          [deleted]   \n",
      "943868  2010-01-01           edman007   \n",
      "943867  2010-01-01      SoManyMinutes   \n",
      "943866  2010-01-01          [deleted]   \n",
      "224124  2010-01-01        PhilxBefore   \n",
      "224123  2010-01-01          btgeekboy   \n",
      "943836  2010-01-01         Terrorpede   \n",
      "943837  2010-01-01           dylanevl   \n",
      "943838  2010-01-01       future_robot   \n",
      "...            ...                ...   \n",
      "632235  2014-12-31          Mantisbog   \n",
      "627784  2014-12-31          [deleted]   \n",
      "627783  2014-12-31          Abraxas01   \n",
      "627782  2014-12-31            edder24   \n",
      "632296  2014-12-31       Stankleberry   \n",
      "627779  2014-12-31            setoxxx   \n",
      "627778  2014-12-31          [deleted]   \n",
      "632310  2014-12-31  bat_in_the_stacks   \n",
      "632311  2014-12-31       Thinblewline   \n",
      "632312  2014-12-31       Thinblewline   \n",
      "632313  2014-12-31      fluffstravels   \n",
      "2596    2014-12-31          Sprinklys   \n",
      "632277  2014-12-31       _OneManArmy_   \n",
      "627777  2014-12-31          iloveit95   \n",
      "632232  2014-12-31        butterscott   \n",
      "627788  2014-12-31       joshgaudette   \n",
      "632239  2014-12-31        tallcircuit   \n",
      "632292  2014-12-31          quadbaser   \n",
      "632291  2014-12-31        tallcircuit   \n",
      "632290  2014-12-31         DEFCON_TWO   \n",
      "2606    2014-12-31         Mozamblake   \n",
      "632225  2014-12-31              darny   \n",
      "632327  2014-12-31             petex3   \n",
      "632285  2014-12-31        tallcircuit   \n",
      "627794  2014-12-31            markdmb   \n",
      "632334  2014-12-31             fsd987   \n",
      "2618    2014-12-31        PlasticGirl   \n",
      "632342  2014-12-31            -o_V_o-   \n",
      "2598    2014-12-31          Sprinklys   \n",
      "627810  2014-12-31       Thinblewline   \n",
      "\n",
      "                                                     body        type  \\\n",
      "943920  Geez dude. I'm on W 4th between 6th and 7th. T...     comment   \n",
      "943870                                                 :(     comment   \n",
      "943871  Maybe try HSBC. Tons of locations somewhat bet...     comment   \n",
      "943872  coco66 in geenpoint.  my coworker's sister ren...     comment   \n",
      "943890  i like The Stone Roulette and Cornelia Street ...     comment   \n",
      "943896             Ask Reddit: Weekly room rental in NYC?  submission   \n",
      "943898  Hey r/nyc what are your favorite music venues/...  submission   \n",
      "943902          2k on my end.  Obviously less the better.     comment   \n",
      "943905  It's a nice place to live. Scientifically the ...     comment   \n",
      "943906  Yeah I know. At this point I might just take i...     comment   \n",
      "943869  I'm finding out that most places are booked so...     comment   \n",
      "943907         We miss another mini reddit meetup dammit.     comment   \n",
      "943909                Buy gold and keep it in your house.     comment   \n",
      "943910  i dont know your situation but i also live in ...     comment   \n",
      "943911          Rubulad got shut down by FDNY last night.     comment   \n",
      "943913  Rudy's tap is disgusting.  They never clean it...     comment   \n",
      "943914  It's an awesome place. :) I just had a tasty b...     comment   \n",
      "943915  Hmm... I've never been there. It's been a long...     comment   \n",
      "943916  Thanks I don't have any plans on moving out ri...     comment   \n",
      "943917  haha thanks. hopefully I'll find a room to ren...     comment   \n",
      "943918  What the heck!  I used to live on Horatio and ...     comment   \n",
      "943908  You're probably best off trying to find a subl...     comment   \n",
      "943868  I like steak but it will take me an hour or so...     comment   \n",
      "943867  Do you like steak? I was thinking of treating ...     comment   \n",
      "943866      Not even Courage Wolf could handle that shit.     comment   \n",
      "224124            Because he posted it in MissingPersons!     comment   \n",
      "224123             No need; it now says she's been found.     comment   \n",
      "943836                                Venice breaks it :(     comment   \n",
      "943837        I take luck wherever I can find it so sure!     comment   \n",
      "943838  I like that its parked on a lawn in staten isl...     comment   \n",
      "...                                                   ...         ...   \n",
      "632235  It seems to me that if some guy stuck his peni...     comment   \n",
      "627784  Mold still a growing problem for hundreds of N...  submission   \n",
      "627783            Easter Egg of a Street for Free Parking  submission   \n",
      "627782                      Does anybody remember Whimit?  submission   \n",
      "632296  The drastic reduction in arrests and tickets s...     comment   \n",
      "627779  Watch \"Fuck her right in the pussy pix11\" on Y...  submission   \n",
      "627778               Where to buy a make masquerade mask?  submission   \n",
      "632310  What are they saying? I get the impression mos...     comment   \n",
      "632311  Throwaway account for paranoia purposes. This ...     comment   \n",
      "632312  These are real cops speaking their mind. Take ...     comment   \n",
      "632313  A while. And I get your viewpoint but I feel l...     comment   \n",
      "2596                             Uhh. Like since forever?     comment   \n",
      "632277  &gt; Cops like it or not brought this on thems...     comment   \n",
      "627777  Driving Through Lincoln Tunnel From New Jersey...  submission   \n",
      "632232  Sorry Mayor is the worst part of this.  Everyo...     comment   \n",
      "627788  Panorama of the Manhattan Bridge taken with iP...  submission   \n",
      "632239  ryan these assholes dont know what's it like t...     comment   \n",
      "632292  You're so fucking wrong and stupid it hurts.&g...     comment   \n",
      "632291  honestly im bitter about people have these exp...     comment   \n",
      "632290  &gt;He strengthened the paid sick leave for NY...     comment   \n",
      "2606    Feel better?\"Every time a person like you ment...     comment   \n",
      "632225  That's dishonest and you're asking for trouble...     comment   \n",
      "632327  Posted as there's no way times sq would fit 1mil      comment   \n",
      "632285  the uber drivers are mostly college kids. sour...     comment   \n",
      "627794  The Terrorism Risk Insurance Act Will Expire o...  submission   \n",
      "632334                                        much better     comment   \n",
      "2618    Yep as others said the best way is to get to U...     comment   \n",
      "632342  &gt; **WeCanNeverBePilots:** Jesus you've got ...     comment   \n",
      "2598    Pretty sure it's something people have been do...     comment   \n",
      "627810                      The attitude of NY's \"finest\"  submission   \n",
      "\n",
      "        emotion   subreddit  \n",
      "943920        4         nyc  \n",
      "943870        4         nyc  \n",
      "943871        4         nyc  \n",
      "943872        4         nyc  \n",
      "943890        0         nyc  \n",
      "943896        2         nyc  \n",
      "943898        4         nyc  \n",
      "943902        0         nyc  \n",
      "943905        4         nyc  \n",
      "943906        0         nyc  \n",
      "943869        0         nyc  \n",
      "943907        0         nyc  \n",
      "943909        4         nyc  \n",
      "943910        0         nyc  \n",
      "943911        0         nyc  \n",
      "943913        0         nyc  \n",
      "943914        4         nyc  \n",
      "943915        4         nyc  \n",
      "943916        0         nyc  \n",
      "943917        4         nyc  \n",
      "943918        4         nyc  \n",
      "943908        0         nyc  \n",
      "943868        4         nyc  \n",
      "943867        4         nyc  \n",
      "943866        0         nyc  \n",
      "224124        2  losangeles  \n",
      "224123        2  losangeles  \n",
      "943836        4         nyc  \n",
      "943837        4         nyc  \n",
      "943838        0         nyc  \n",
      "...         ...         ...  \n",
      "632235        4         nyc  \n",
      "627784        4         nyc  \n",
      "627783        4         nyc  \n",
      "627782        4         nyc  \n",
      "632296        0         nyc  \n",
      "627779        0         nyc  \n",
      "627778        0         nyc  \n",
      "632310        0         nyc  \n",
      "632311        0         nyc  \n",
      "632312        0         nyc  \n",
      "632313        0         nyc  \n",
      "2596          0  losangeles  \n",
      "632277        0         nyc  \n",
      "627777        4         nyc  \n",
      "632232        0         nyc  \n",
      "627788        4         nyc  \n",
      "632239        0         nyc  \n",
      "632292        0         nyc  \n",
      "632291        0         nyc  \n",
      "632290        0         nyc  \n",
      "2606          0  losangeles  \n",
      "632225        0         nyc  \n",
      "632327        0         nyc  \n",
      "632285        0         nyc  \n",
      "627794        0         nyc  \n",
      "632334        4         nyc  \n",
      "2618          0  losangeles  \n",
      "632342        0         nyc  \n",
      "2598          0  losangeles  \n",
      "627810        4         nyc  \n",
      "\n",
      "[948371 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# turn them into year-month-day\n",
    "for i, row in reddit_master_collection.iterrows():\n",
    "    #datetime.datetime.strptime(when, '%Y-%m-%d').date()\n",
    "    date = reddit_master_collection.at[i, 'created_utc'].date()\n",
    "    reddit_master_collection.at[i, 'created_utc'] = date\n",
    "    if i % 20000 == 0:\n",
    "        print('Iteration', i, end='\\r')\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       created_utc             author  \\\n",
      "943920  2010-01-01            DesCo83   \n",
      "943870  2010-01-01           edman007   \n",
      "943871  2010-01-01                jba   \n",
      "943872  2010-01-01         krugerlive   \n",
      "943890  2010-01-01             coasts   \n",
      "943896  2010-01-01          [deleted]   \n",
      "943898  2010-01-01             coasts   \n",
      "943902  2010-01-01            brianoh   \n",
      "943905  2010-01-01           beerbabe   \n",
      "943906  2010-01-01            DesCo83   \n",
      "943869  2010-01-01      SoManyMinutes   \n",
      "943907  2010-01-01            bmeckel   \n",
      "943909  2010-01-01           middkidd   \n",
      "943910  2010-01-01      norwegianrich   \n",
      "943911  2010-01-01        elsagacious   \n",
      "943913  2010-01-01            brianoh   \n",
      "943914  2010-01-01          zwangaman   \n",
      "943915  2010-01-01           beerbabe   \n",
      "943916  2010-01-01            DesCo83   \n",
      "943917  2010-01-01       mrcassavetes   \n",
      "943918  2010-01-01            brianoh   \n",
      "943908  2010-01-01          [deleted]   \n",
      "943868  2010-01-01           edman007   \n",
      "943867  2010-01-01      SoManyMinutes   \n",
      "943866  2010-01-01          [deleted]   \n",
      "224124  2010-01-01        PhilxBefore   \n",
      "224123  2010-01-01          btgeekboy   \n",
      "943836  2010-01-01         Terrorpede   \n",
      "943837  2010-01-01           dylanevl   \n",
      "943838  2010-01-01       future_robot   \n",
      "...            ...                ...   \n",
      "632235  2014-12-31          Mantisbog   \n",
      "627784  2014-12-31          [deleted]   \n",
      "627783  2014-12-31          Abraxas01   \n",
      "627782  2014-12-31            edder24   \n",
      "632296  2014-12-31       Stankleberry   \n",
      "627779  2014-12-31            setoxxx   \n",
      "627778  2014-12-31          [deleted]   \n",
      "632310  2014-12-31  bat_in_the_stacks   \n",
      "632311  2014-12-31       Thinblewline   \n",
      "632312  2014-12-31       Thinblewline   \n",
      "632313  2014-12-31      fluffstravels   \n",
      "2596    2014-12-31          Sprinklys   \n",
      "632277  2014-12-31       _OneManArmy_   \n",
      "627777  2014-12-31          iloveit95   \n",
      "632232  2014-12-31        butterscott   \n",
      "627788  2014-12-31       joshgaudette   \n",
      "632239  2014-12-31        tallcircuit   \n",
      "632292  2014-12-31          quadbaser   \n",
      "632291  2014-12-31        tallcircuit   \n",
      "632290  2014-12-31         DEFCON_TWO   \n",
      "2606    2014-12-31         Mozamblake   \n",
      "632225  2014-12-31              darny   \n",
      "632327  2014-12-31             petex3   \n",
      "632285  2014-12-31        tallcircuit   \n",
      "627794  2014-12-31            markdmb   \n",
      "632334  2014-12-31             fsd987   \n",
      "2618    2014-12-31        PlasticGirl   \n",
      "632342  2014-12-31            -o_V_o-   \n",
      "2598    2014-12-31          Sprinklys   \n",
      "627810  2014-12-31       Thinblewline   \n",
      "\n",
      "                                                     body        type  \\\n",
      "943920  Geez dude. I'm on W 4th between 6th and 7th. T...     comment   \n",
      "943870                                                 :(     comment   \n",
      "943871  Maybe try HSBC. Tons of locations somewhat bet...     comment   \n",
      "943872  coco66 in geenpoint.  my coworker's sister ren...     comment   \n",
      "943890  i like The Stone Roulette and Cornelia Street ...     comment   \n",
      "943896             Ask Reddit: Weekly room rental in NYC?  submission   \n",
      "943898  Hey r/nyc what are your favorite music venues/...  submission   \n",
      "943902          2k on my end.  Obviously less the better.     comment   \n",
      "943905  It's a nice place to live. Scientifically the ...     comment   \n",
      "943906  Yeah I know. At this point I might just take i...     comment   \n",
      "943869  I'm finding out that most places are booked so...     comment   \n",
      "943907         We miss another mini reddit meetup dammit.     comment   \n",
      "943909                Buy gold and keep it in your house.     comment   \n",
      "943910  i dont know your situation but i also live in ...     comment   \n",
      "943911          Rubulad got shut down by FDNY last night.     comment   \n",
      "943913  Rudy's tap is disgusting.  They never clean it...     comment   \n",
      "943914  It's an awesome place. :) I just had a tasty b...     comment   \n",
      "943915  Hmm... I've never been there. It's been a long...     comment   \n",
      "943916  Thanks I don't have any plans on moving out ri...     comment   \n",
      "943917  haha thanks. hopefully I'll find a room to ren...     comment   \n",
      "943918  What the heck!  I used to live on Horatio and ...     comment   \n",
      "943908  You're probably best off trying to find a subl...     comment   \n",
      "943868  I like steak but it will take me an hour or so...     comment   \n",
      "943867  Do you like steak? I was thinking of treating ...     comment   \n",
      "943866      Not even Courage Wolf could handle that shit.     comment   \n",
      "224124            Because he posted it in MissingPersons!     comment   \n",
      "224123             No need; it now says she's been found.     comment   \n",
      "943836                                Venice breaks it :(     comment   \n",
      "943837        I take luck wherever I can find it so sure!     comment   \n",
      "943838  I like that its parked on a lawn in staten isl...     comment   \n",
      "...                                                   ...         ...   \n",
      "632235  It seems to me that if some guy stuck his peni...     comment   \n",
      "627784  Mold still a growing problem for hundreds of N...  submission   \n",
      "627783            Easter Egg of a Street for Free Parking  submission   \n",
      "627782                      Does anybody remember Whimit?  submission   \n",
      "632296  The drastic reduction in arrests and tickets s...     comment   \n",
      "627779  Watch \"Fuck her right in the pussy pix11\" on Y...  submission   \n",
      "627778               Where to buy a make masquerade mask?  submission   \n",
      "632310  What are they saying? I get the impression mos...     comment   \n",
      "632311  Throwaway account for paranoia purposes. This ...     comment   \n",
      "632312  These are real cops speaking their mind. Take ...     comment   \n",
      "632313  A while. And I get your viewpoint but I feel l...     comment   \n",
      "2596                             Uhh. Like since forever?     comment   \n",
      "632277  &gt; Cops like it or not brought this on thems...     comment   \n",
      "627777  Driving Through Lincoln Tunnel From New Jersey...  submission   \n",
      "632232  Sorry Mayor is the worst part of this.  Everyo...     comment   \n",
      "627788  Panorama of the Manhattan Bridge taken with iP...  submission   \n",
      "632239  ryan these assholes dont know what's it like t...     comment   \n",
      "632292  You're so fucking wrong and stupid it hurts.&g...     comment   \n",
      "632291  honestly im bitter about people have these exp...     comment   \n",
      "632290  &gt;He strengthened the paid sick leave for NY...     comment   \n",
      "2606    Feel better?\"Every time a person like you ment...     comment   \n",
      "632225  That's dishonest and you're asking for trouble...     comment   \n",
      "632327  Posted as there's no way times sq would fit 1mil      comment   \n",
      "632285  the uber drivers are mostly college kids. sour...     comment   \n",
      "627794  The Terrorism Risk Insurance Act Will Expire o...  submission   \n",
      "632334                                        much better     comment   \n",
      "2618    Yep as others said the best way is to get to U...     comment   \n",
      "632342  &gt; **WeCanNeverBePilots:** Jesus you've got ...     comment   \n",
      "2598    Pretty sure it's something people have been do...     comment   \n",
      "627810                      The attitude of NY's \"finest\"  submission   \n",
      "\n",
      "        emotion   subreddit  \n",
      "943920        4         nyc  \n",
      "943870        4         nyc  \n",
      "943871        4         nyc  \n",
      "943872        4         nyc  \n",
      "943890        0         nyc  \n",
      "943896        2         nyc  \n",
      "943898        4         nyc  \n",
      "943902        0         nyc  \n",
      "943905        4         nyc  \n",
      "943906        0         nyc  \n",
      "943869        0         nyc  \n",
      "943907        0         nyc  \n",
      "943909        4         nyc  \n",
      "943910        0         nyc  \n",
      "943911        0         nyc  \n",
      "943913        0         nyc  \n",
      "943914        4         nyc  \n",
      "943915        4         nyc  \n",
      "943916        0         nyc  \n",
      "943917        4         nyc  \n",
      "943918        4         nyc  \n",
      "943908        0         nyc  \n",
      "943868        4         nyc  \n",
      "943867        4         nyc  \n",
      "943866        0         nyc  \n",
      "224124        2  losangeles  \n",
      "224123        2  losangeles  \n",
      "943836        4         nyc  \n",
      "943837        4         nyc  \n",
      "943838        0         nyc  \n",
      "...         ...         ...  \n",
      "632235        4         nyc  \n",
      "627784        4         nyc  \n",
      "627783        4         nyc  \n",
      "627782        4         nyc  \n",
      "632296        0         nyc  \n",
      "627779        0         nyc  \n",
      "627778        0         nyc  \n",
      "632310        0         nyc  \n",
      "632311        0         nyc  \n",
      "632312        0         nyc  \n",
      "632313        0         nyc  \n",
      "2596          0  losangeles  \n",
      "632277        0         nyc  \n",
      "627777        4         nyc  \n",
      "632232        0         nyc  \n",
      "627788        4         nyc  \n",
      "632239        0         nyc  \n",
      "632292        0         nyc  \n",
      "632291        0         nyc  \n",
      "632290        0         nyc  \n",
      "2606          0  losangeles  \n",
      "632225        0         nyc  \n",
      "632327        0         nyc  \n",
      "632285        0         nyc  \n",
      "627794        0         nyc  \n",
      "632334        4         nyc  \n",
      "2618          0  losangeles  \n",
      "632342        0         nyc  \n",
      "2598          0  losangeles  \n",
      "627810        4         nyc  \n",
      "\n",
      "[948371 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reddit_master_collection.sort_values(by=[\"created_utc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the work for showing the change over time by subreddit/city\n",
    "\n",
    "years = ['2011', '2012', '2013', '2014', '2015']\n",
    "subreddits = [\"nyc\", \"losangeles\", \"chicago\", \"houston\", \"phoenix\"]\n",
    "\n",
    "bar_chart_source = reddit_master_collection\n",
    "\n",
    "frames_by_subreddit = []\n",
    "for subreddit in subreddits:\n",
    "    frames_by_subreddit.append(bar_chart_source[bar_chart_source.subreddit == subreddit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 940000\r"
     ]
    }
   ],
   "source": [
    "# we need only the year now\n",
    "bokeh_fixed_year_frame = bar_chart_source\n",
    "for i, row in bokeh_fixed_year_frame.iterrows():\n",
    "    date = str(bokeh_fixed_year_frame.at[i, 'created_utc']).split('-')[0]\n",
    "    bokeh_fixed_year_frame.at[i, 'created_utc'] = date\n",
    "    if i % 20000 == 0:\n",
    "        print('Iteration', i, end='\\r')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be reusable foir all the bar graphs i'm going to make\n",
    "\n",
    "bokeh_years = ['2010', '2011', '2012', '2013', '2014']\n",
    "\n",
    "def showBarGraphByYear(bokeh_source, height, title, xaxis, yaxis, filename):\n",
    "    source = ColumnDataSource(data=bokeh_source) \n",
    "\n",
    "    p = figure(x_range=bokeh_years, y_range=(0, height), plot_height=400, title=title,\n",
    "               toolbar_location=None, tools=\"\")\n",
    "\n",
    "    p.vbar(x=dodge('years', -0.25, range=p.x_range), top='Negative', width=0.2, source=source,\n",
    "           color=\"#ff4d4d\", legend=value(\"Negative\"))\n",
    "\n",
    "    p.vbar(x=dodge('years',  0.0,  range=p.x_range), top='Neutral', width=0.2, source=source,\n",
    "           color=\"#80bfff\", legend=value(\"Neutral\"))\n",
    "\n",
    "    p.vbar(x=dodge('years',  0.25, range=p.x_range), top='Positive', width=0.2, source=source,\n",
    "           color=\"#00cc44\", legend=value(\"Positive\"))\n",
    "\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.legend.location = \"top_left\"\n",
    "    p.legend.orientation = \"horizontal\"\n",
    "    if not xaxis is None:\n",
    "        p.xaxis.axis_label = xaxis\n",
    "    if not yaxis is None:\n",
    "        p.yaxis.axis_label = yaxis\n",
    "    \n",
    "    p.output_backend = \"svg\"\n",
    "    export_svgs(p, filename=filename)\n",
    "    \n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar graph by year - all subreddit content\n",
    "\n",
    "height = 250\n",
    "title = \"Emotional Rating of Reddit Content by Year\"\n",
    "xaxis = \"'Reddit Content' is aggregated from subreddits: nyc, losangeles, chicago, houston, phoenix\"\n",
    "yaxis = \"Total number of posts (in thousands)\"\n",
    "\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "for year in bokeh_years:\n",
    "    total_negative = 0\n",
    "    total_neutral = 0\n",
    "    total_positive = 0\n",
    "    emotions = bokeh_fixed_year_frame[bokeh_fixed_year_frame.created_utc == year]['emotion']\n",
    "    for emotion in emotions:\n",
    "        if emotion == 0:\n",
    "            total_negative += 1\n",
    "        if emotion == 2:\n",
    "            total_neutral += 1\n",
    "        else:\n",
    "            total_positive += 1\n",
    "    negative.append(total_negative/1000)\n",
    "    neutral.append(total_neutral/1000)\n",
    "    positive.append(total_positive/1000)\n",
    "\n",
    "\n",
    "# this is overall, not by subreddit\n",
    "by_year_source = { 'years': bokeh_years,\n",
    "               'Negative': negative,\n",
    "               'Neutral': neutral,\n",
    "               'Positive': positive}\n",
    "\n",
    "showBarGraphByYear(by_year_source, height, title, xaxis, yaxis, \"reddit_emotions_by_year.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar graph by year by subreddit\n",
    "\n",
    "frames_by_subreddit = []\n",
    "for subreddit in subreddits:\n",
    "    frames_by_subreddit.append(bokeh_fixed_year_frame[bokeh_fixed_year_frame.subreddit == subreddit])\n",
    "\n",
    "amounts_by_subreddit = []\n",
    "for subreddit_frame in frames_by_subreddit:\n",
    "    negative = []\n",
    "    neutral = []\n",
    "    positive = []\n",
    "    for year in bokeh_years:\n",
    "        total_negative = 0\n",
    "        total_neutral = 0\n",
    "        total_positive = 0\n",
    "        emotions = subreddit_frame[subreddit_frame.created_utc == year]['emotion']\n",
    "        for emotion in emotions:\n",
    "            if emotion == 0:\n",
    "                total_negative += 1\n",
    "            if emotion == 2:\n",
    "                total_neutral += 1\n",
    "            else:\n",
    "                total_positive += 1\n",
    "        negative.append(total_negative)\n",
    "        neutral.append(total_neutral)\n",
    "        positive.append(total_positive)\n",
    "    \n",
    "    current_subreddit = str(subreddit_frame[\"subreddit\"].iloc[0])\n",
    "    height = max([max(negative), max(neutral), max(positive)]) * 1.2\n",
    "    title = 'Emotional Rating of %s Subreddit Content by Year' %(current_subreddit)\n",
    "    xaxis = \"'Reddit Content' is an aggregation of submissions and comments\"\n",
    "    yaxis = \"Total number of posts\"\n",
    "    \n",
    "    amounts_by_subreddit.append([current_subreddit, negative, neutral, positive])\n",
    "    \n",
    "    by_subreddit_by_year_source = { 'years': bokeh_years,\n",
    "               'Negative': negative,\n",
    "               'Neutral': neutral,\n",
    "               'Positive': positive}\n",
    "    \n",
    "    file_name = '%s_emotions_by_year.svg' %(current_subreddit)\n",
    "    showBarGraphByYear(by_subreddit_by_year_source, height, title, xaxis, yaxis, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateChange(old, new):\n",
    "    return round((((new - old)/old)*100),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nyc', [-0.0018, 0.188, 1.0086, 0.9074]], ['losangeles', [0.0052, 0.0672, 0.6789, 0.8216]], ['chicago', [0.001, 0.0707, 0.3959, 0.4256]], ['houston', [-0.278, 0.2307, 1.2246, 1.6921]], ['phoenix', [0.0887, 0.1646, 1.4322, 2.0263]]]\n"
     ]
    }
   ],
   "source": [
    "# calculate percentage in change of U.S. population -- from census data\n",
    "\n",
    "census_data_file = \"D:\\social_media_analytics\\\\reddit_content\\collection\\census_data.csv\"\n",
    "\n",
    "census_df = pandas.read_csv(census_data_file)\n",
    "\n",
    "census_change_over_time = []\n",
    "for subreddit in subreddits:\n",
    "    subreddit_frame = census_df[census_df.subreddit == subreddit]\n",
    "    previous_year_value = None\n",
    "    subreddit_change_over_time = []\n",
    "    for year in bokeh_years:\n",
    "        if not previous_year_value is None:\n",
    "            current_year_population = subreddit_frame[year].iloc[0]\n",
    "            subreddit_change_over_time.append(calculateChange(previous_year_value,current_year_population))\n",
    "            \n",
    "        previous_year_value = subreddit_frame[year].iloc[0]\n",
    "        \n",
    "    census_change_over_time.append([subreddit, subreddit_change_over_time])\n",
    "\n",
    "print(census_change_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nyc', [96.6203, 9.8966, -0.4498, 0.8712]], ['nyc', [117.177, 11.7263, 3.478, -0.9816]], ['nyc', [99.8447, 7.2211, -0.3283, -3.6972]], ['losangeles', [291.5081, 30.3161, 5.6121, -10.3367]], ['losangeles', [306.0748, 45.5696, 11.8577, -17.1025]], ['losangeles', [274.7439, 32.0106, 5.125, -14.571]], ['chicago', [90.5024, 12.4321, 4.4256, -19.2566]], ['chicago', [114.0766, 22.0936, 17.1478, -21.8095]], ['chicago', [93.6153, 12.0428, 6.278, -22.7984]], ['houston', [201.9693, 58.3485, 10.4839, -21.4121]], ['houston', [201.2945, 57.6799, 24.3869, -20.7558]], ['houston', [184.1717, 56.2139, 13.0797, -21.3664]], ['phoenix', [5430.0, 281.9168, 72.3485, 4.5604]], ['phoenix', [4500.0, 339.1304, 47.2772, -1.3445]], ['phoenix', [5391.1111, 269.7693, 63.9378, 4.3661]]]\n"
     ]
    }
   ],
   "source": [
    "# pass in a list of [city,negative yearly count, neuitral, positive]\n",
    "#def getEmotionChangeOverTimeByYear(source_list):\n",
    "\n",
    "subreddit_yearly_change = []\n",
    "for group in amounts_by_subreddit:\n",
    "    subreddit = group[0]\n",
    "    negative = group[1]\n",
    "    neutral = group[2]\n",
    "    positive = group[3]\n",
    "    i = 0\n",
    "    subreddit_negative_change = []\n",
    "    subreddit_neutral_change = []\n",
    "    subreddit_positive_change = []\n",
    "    for year in bokeh_years:\n",
    "        if not year == '2010':\n",
    "            subreddit_negative_change.append(calculateChange(negative[i-1], negative[i]))\n",
    "            subreddit_neutral_change.append(calculateChange(neutral[i-1], neutral[i]))\n",
    "            subreddit_positive_change.append(calculateChange(positive[i-1], positive[i]))\n",
    "        i += 1\n",
    "    subreddit_yearly_change.append([subreddit, subreddit_negative_change])\n",
    "    subreddit_yearly_change.append([subreddit, subreddit_neutral_change])\n",
    "    subreddit_yearly_change.append([subreddit, subreddit_positive_change])\n",
    "print(subreddit_yearly_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
