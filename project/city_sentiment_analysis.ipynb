{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import nltk\n",
    "import multiprocessing as mp\n",
    "import threading_jobs as tj\n",
    "# uncomment to download nltk library\n",
    "#nltk.download()\n",
    "os.environ[\"NLTK_DATA\"] = \"D:\\nltk_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --prefix {sys.prefix} -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                               post\n",
       "0        4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1        4  Reading my kindle2...  Love it... Lee childs i...\n",
       "2        4  Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3        4  @kenburbary You'll love your Kindle2. I've had...\n",
       "4        4  @mikefish  Fair enough. But i have the Kindle2..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this section gets the initial data frame from the sentiment140 file\n",
    "\n",
    "# load the sentiment 140 dataset to pandas\n",
    "\n",
    "# this takes way too long, literally days to process, and jupyter crashed twice on me and left me having to restart the processing....\n",
    "# sentiment140dataset = \"training.1600000.processed.noemoticon.csv\"\n",
    "sentiment140dataset = \"testdata.manual.2009.06.14.csv\"\n",
    "\n",
    "# this has 1.6 million rows\n",
    "s140tweets = pandas.read_csv(sentiment140dataset, encoding='utf-8')\n",
    "\n",
    "s140tweets.drop(\"id\", axis=1, inplace=True)\n",
    "s140tweets.drop(\"date\", axis=1, inplace=True)\n",
    "s140tweets.drop(\"topic\", axis=1, inplace=True)\n",
    "s140tweets.drop(\"account\", axis=1, inplace=True)\n",
    "\n",
    "# the above drops will leave us with only the \"emotion\" and \"post\" columns,\n",
    "# which exactly what we care about\n",
    "\n",
    "s140tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                               post\n",
       "0        4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1        4  Reading my kindle2...  Love it... Lee childs i...\n",
       "2        4  Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3        4  @kenburbary You'll love your Kindle2. I've had...\n",
       "4        4  @mikefish  Fair enough. But i have the Kindle2..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also are only going to take 100,000 posts, instead of 1.6 million, because 1.6 million is too much\n",
    "\n",
    "#zero_tweets = s140tweets[s140tweets.emotion == 0]\n",
    "#four_tweets = s140tweets[s140tweets.emotion == 4]\n",
    "#final_tweet_df = pandas.concat([zero_tweets[:30000], four_tweets[:30000]])\n",
    "\n",
    "# actually i'm using the smaller set now because of how much data was in the other one - it was too cumbersome\n",
    "final_tweet_df = s140tweets\n",
    "final_tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section gets the data prepared to be put through the classifier\n",
    "\n",
    "# this gives the (post,emotion) tuple\n",
    "def translate_dataframe_to_post_format(frame):\n",
    "    posts = []\n",
    "    for index, row in frame.iterrows():\n",
    "        words = row['post']\n",
    "        post = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "        posts.append((post, row['emotion']))\n",
    "    return posts\n",
    "\n",
    "def get_words_in_tweets(posts):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in posts:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "posts = translate_dataframe_to_post_format(final_tweet_df)\n",
    "\n",
    "words = get_words_in_tweets(posts)\n",
    "word_features = get_word_features(words)\n",
    "\n",
    "# print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section builds the classifier\n",
    "# we currently have about 1.6 million posts to build our classifier with\n",
    "# but I took 30k positive and 30k negative posts to classify with, because 1.6m took more than a full day to process\n",
    "# so I stopped it\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(hate) = True                0 : 4      =     10.6 : 1.0\n",
      "        contains(jquery) = True                2 : 0      =     10.6 : 1.0\n",
      "          contains(time) = True                0 : 4      =      9.8 : 1.0\n",
      "           contains(not) = True                0 : 2      =      9.2 : 1.0\n",
      "          contains(love) = True                4 : 0      =      8.8 : 1.0\n",
      "        contains(warner) = True                0 : 2      =      7.7 : 1.0\n",
      "         contains(great) = True                4 : 2      =      6.4 : 1.0\n",
      "        contains(lebron) = True                4 : 2      =      6.4 : 1.0\n",
      "          contains(good) = True                4 : 2      =      5.7 : 1.0\n",
      "           contains(his) = True                4 : 0      =      5.5 : 1.0\n",
      "           contains(are) = True                0 : 2      =      5.5 : 1.0\n",
      "           contains(one) = True                4 : 0      =      4.9 : 1.0\n",
      "           contains(san) = True                2 : 4      =      4.8 : 1.0\n",
      "       contains(safeway) = True                2 : 4      =      4.8 : 1.0\n",
      "           contains(why) = True                0 : 4      =      4.5 : 1.0\n",
      "         contains(&amp;) = True                4 : 2      =      4.3 : 1.0\n",
      "        contains(museum) = True                2 : 0      =      4.3 : 1.0\n",
      "           contains(see) = True                2 : 0      =      4.3 : 1.0\n",
      "          contains(they) = True                0 : 4      =      4.3 : 1.0\n",
      "         contains(night) = True                4 : 0      =      4.3 : 1.0\n",
      "          contains(very) = True                4 : 0      =      4.2 : 1.0\n",
      "       contains(kindle2) = True                4 : 0      =      4.2 : 1.0\n",
      "         contains(still) = True                0 : 2      =      3.9 : 1.0\n",
      "       contains(malcolm) = True                4 : 2      =      3.8 : 1.0\n",
      "           contains(...) = True                2 : 0      =      3.8 : 1.0\n",
      "         contains(three) = True                2 : 0      =      3.8 : 1.0\n",
      "          contains(down) = True                0 : 4      =      3.8 : 1.0\n",
      "         contains(could) = True                0 : 4      =      3.8 : 1.0\n",
      "          contains(wish) = True                0 : 4      =      3.8 : 1.0\n",
      "        contains(google) = True                4 : 0      =      3.7 : 1.0\n",
      "           contains(it!) = True                4 : 0      =      3.6 : 1.0\n",
      "         contains(happy) = True                4 : 0      =      3.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# this section shows some information about the trained classifier\n",
    "\n",
    "print(classifier.show_most_informative_features(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows added for processing for nyc 2010\n",
      "Rows added for processing for nyc 2010\n",
      "Rows added for processing for nyc 2010\n",
      "Rows added for processing for nyc 2010\n",
      "Rows added for processing for nyc 2010\n",
      "Rows added for processing for nyc 2010\n",
      "Rows added for processing for nyc 2010\n",
      "Rows added for processing for nyc 2010\n",
      "Rows processed for nyc 2010\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows added for processing for nyc 2011\n",
      "Rows processed for nyc 2011\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows added for processing for nyc 2012\n",
      "Rows processed for nyc 2012\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows added for processing for nyc 2013\n",
      "Rows processed for nyc 2013\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows added for processing for nyc 2014\n",
      "Rows processed for nyc 2014\n",
      "Rows added for processing for chicago 2010\n",
      "Rows added for processing for chicago 2010\n",
      "Rows added for processing for chicago 2010\n",
      "Rows added for processing for chicago 2010\n",
      "Rows added for processing for chicago 2010\n",
      "Rows processed for chicago 2010\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows added for processing for chicago 2011\n",
      "Rows processed for chicago 2011\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows added for processing for chicago 2012\n",
      "Rows processed for chicago 2012\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows added for processing for chicago 2013\n",
      "Rows processed for chicago 2013\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows added for processing for chicago 2014\n",
      "Rows processed for chicago 2014\n",
      "Rows added for processing for houston 2010\n",
      "Rows added for processing for houston 2010\n",
      "Rows processed for houston 2010\n",
      "Rows added for processing for houston 2011\n",
      "Rows added for processing for houston 2011\n",
      "Rows added for processing for houston 2011\n",
      "Rows added for processing for houston 2011\n",
      "Rows added for processing for houston 2011\n",
      "Rows processed for houston 2011\n",
      "Rows added for processing for houston 2012\n",
      "Rows added for processing for houston 2012\n",
      "Rows added for processing for houston 2012\n",
      "Rows added for processing for houston 2012\n",
      "Rows added for processing for houston 2012\n",
      "Rows added for processing for houston 2012\n",
      "Rows added for processing for houston 2012\n",
      "Rows added for processing for houston 2012\n",
      "Rows processed for houston 2012\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows added for processing for houston 2013\n",
      "Rows processed for houston 2013\n",
      "Rows added for processing for houston 2014\n",
      "Rows added for processing for houston 2014\n",
      "Rows added for processing for houston 2014\n",
      "Rows added for processing for houston 2014\n",
      "Rows added for processing for houston 2014\n",
      "Rows added for processing for houston 2014\n",
      "Rows added for processing for houston 2014\n",
      "Rows processed for houston 2014\n",
      "Rows added for processing for phoenix 2010\n",
      "Rows processed for phoenix 2010\n",
      "Rows added for processing for phoenix 2011\n",
      "Rows processed for phoenix 2011\n",
      "Rows added for processing for phoenix 2012\n",
      "Rows added for processing for phoenix 2012\n",
      "Rows added for processing for phoenix 2012\n",
      "Rows processed for phoenix 2012\n",
      "Rows added for processing for phoenix 2013\n",
      "Rows added for processing for phoenix 2013\n",
      "Rows added for processing for phoenix 2013\n",
      "Rows added for processing for phoenix 2013\n",
      "Rows processed for phoenix 2013\n",
      "Rows added for processing for phoenix 2014\n",
      "Rows added for processing for phoenix 2014\n",
      "Rows added for processing for phoenix 2014\n",
      "Rows added for processing for phoenix 2014\n",
      "Rows processed for phoenix 2014\n",
      "Rows added for processing for losangeles 2010\n",
      "Rows added for processing for losangeles 2010\n",
      "Rows added for processing for losangeles 2010\n",
      "Rows processed for losangeles 2010\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n",
      "Rows added for processing for losangeles 2011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed for losangeles 2011\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows added for processing for losangeles 2012\n",
      "Rows processed for losangeles 2012\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows added for processing for losangeles 2013\n",
      "Rows processed for losangeles 2013\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows added for processing for losangeles 2014\n",
      "Rows processed for losangeles 2014\n"
     ]
    }
   ],
   "source": [
    "# classify the submissions and comments retieved from Reddit\n",
    "# write it to a csv\n",
    "\n",
    "def getFileName(subreddit, year):\n",
    "    return 'D:\\social_media_analytics\\\\reddit_content\\%s\\%s_%s_submissions_and_comments.csv' %(subreddit, year, subreddit)\n",
    "\n",
    "\n",
    "years = [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\"]\n",
    "subreddits = [\"nyc\", \"chicago\", \"houston\", \"phoenix\", \"losangeles\"]\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    "    p = mp.Pool(processes = (mp.cpu_count()))\n",
    "\n",
    "    final_tweet_df = None\n",
    "    for subreddit in subreddits:\n",
    "        for year in years:\n",
    "            file_name = getFileName(subreddit, year)\n",
    "            \n",
    "            reader = pandas.read_csv(file_name, chunksize=5000, encoding='ANSI')\n",
    "            \n",
    "            funclist = []\n",
    "            for frame in reader:\n",
    "                frame = frame[frame.body != '[deleted]']\n",
    "                frame[\"emotion\"] = -1\n",
    "                frame[\"subreddit\"] = subreddit\n",
    "                # process each data frame\n",
    "                f = p.apply_async(tj.process_frame, [frame, classifier])\n",
    "                funclist.append(f)\n",
    "                print('Rows added for processing for %s %s' %(subreddit, year))\n",
    "            for f in funclist:\n",
    "                if not final_tweet_df is None:\n",
    "                    final_tweet_df = pandas.concat([f.get(), final_tweet_df])\n",
    "                else:\n",
    "                    final_tweet_df = f.get()\n",
    "            print('Rows processed for %s %s' %(subreddit, year))\n",
    "\n",
    "\n",
    "        # run the CSV update per subreddit, just in case\n",
    "        # this file will either be created or wipe the existing and make a new one\n",
    "        final_tweet_df.to_csv(path_or_buf =\"D:\\social_media_analytics\\\\reddit_content\\collection\\sentiment_data_2010-2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       created_utc               author  \\\n",
      "55000   1418286349            LaunchGap   \n",
      "55001   1418286640  theultimateusername   \n",
      "55002   1418288214             kneemahp   \n",
      "55003   1418288816          PlasticGirl   \n",
      "55004   1418293495         Front_Street   \n",
      "\n",
      "                                                    body     type  emotion  \\\n",
      "55000  Haven't been on 405 during rush hour in a whil...  comment        4   \n",
      "55001                                          Stairs...  comment        4   \n",
      "55002    Holy shit wth is rosedale?! Looks really nice.   comment        0   \n",
      "55003     There's a $2.89 in Pico Union and Bunker Hill.  comment        4   \n",
      "55004  I hear that place by the staples center is leg...  comment        0   \n",
      "\n",
      "        subreddit  \n",
      "55000  losangeles  \n",
      "55001  losangeles  \n",
      "55002  losangeles  \n",
      "55003  losangeles  \n",
      "55004  losangeles  \n"
     ]
    }
   ],
   "source": [
    "print(final_tweet_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_utc    948480\n",
      "author         948480\n",
      "body           948477\n",
      "type           948480\n",
      "emotion        948480\n",
      "subreddit      948480\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_tweet_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
