{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import urllib.request\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas\n",
    "import multiprocessing as mp\n",
    "import threading_jobs as tj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the following, string replace these values:\n",
    "# %subreddit%\n",
    "# %before_epoch%\n",
    "# %after_epoch%\n",
    "# %size%\n",
    "\n",
    "# only difference is submission and num_comments\n",
    "def getBaseSubmissionURL():\n",
    "    return \"https://api.pushshift.io/reddit/search/submission/?subreddit=%subreddit%&after=%after_epoch%&before=%before_epoch%&sort_type=num_comments&size=%size%\"\n",
    "\n",
    "# only difference is comment and score\n",
    "def getBaseCommentURL():\n",
    "    return \"https://api.pushshift.io/reddit/search/comment/?subreddit=%subreddit%&after=%after_epoch%&before=%before_epoch%&sort_type=score&size=%size%\"\n",
    "\n",
    "# build a request url with given input\n",
    "def buildBasicRequest(baseUrl, subreddit, before_epoch, after_epoch, size):\n",
    "    return baseUrl.replace(\"%subreddit%\", subreddit).replace(\"%after_epoch%\", after_epoch).replace(\"%before_epoch%\", before_epoch).replace(\"%size%\", size)\n",
    "\n",
    "subreddits = [\"nyc\", \"losangeles\", \"chicago\", \"houston\", \"phoenix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs\n",
    "def getEpochForDate(year, month, day):\n",
    "    return str(datetime.datetime(year,month,day,0,0).timestamp()).rstrip('0').rstrip('.')\n",
    "\n",
    "print(getEpochForDate(2010,12,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is how we check what file to write to\n",
    "# this is also going to initialize the files that we need if it isn't there\n",
    "\n",
    "def getFileName(subreddit, year):\n",
    "    # check file exists in 'D:\\social_media_analytics\\reddit_content\\chicago'\n",
    "    # if it does not, create the file with initial headers for .csv\n",
    "    # regardless, return \"D:\\social_media_analytics\\reddit_content\\{subreddit}\\{year}_reddit_submissions_and_comments.csv\"\n",
    "    file_name = 'D:\\social_media_analytics\\\\reddit_content\\%s\\%s_%s_submissions_and_comments.csv' %(subreddit, year, subreddit)\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(\"writing new file for \" + subreddit + \":\" + str(year))\n",
    "        f = open(file_name, \"+a\")\n",
    "        f.write(\"created_utc,author,body,type\\n\")\n",
    "        f.close()\n",
    "    return file_name\n",
    "\n",
    "\n",
    "# print(getFileName(\"houston\", 2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the meat, here is where the calls happen\n",
    "\n",
    "# the approach is to enter this with the timeframe,\n",
    "# then we're going to make both calls for each of our five cities\n",
    "\n",
    "# we start with submissions, pull back up to 100 - sorted by most comments\n",
    "# we do size=250-submissions_result_count for comments\n",
    "\n",
    "# write everything to a file for the given year\n",
    "# we are writing to {year}_reddit_submissions_and_comments.csv\n",
    "\n",
    "# here is the data we care about in our results:\n",
    "# \"created_utc\" : epoch time\n",
    "# \"author\"      : string, could be repeats or \"[deleted]\"\n",
    "# \"body\"        : string\n",
    "# \"type\"        : string, could be submission or comment\n",
    "\n",
    "def parsePosts(subreddit, posts, content_name, comment_or_submission):\n",
    "    data = json.loads(posts.decode(\"utf-8\"))\n",
    "    proper_list_of_posts = data[\"data\"]\n",
    "    results = []\n",
    "    for post in proper_list_of_posts:\n",
    "        author = post[\"author\"]\n",
    "        created_utc = post[\"created_utc\"]\n",
    "        body = post[content_name]\n",
    "        try:\n",
    "            body = body.replace(\"\\r\", \"\")\n",
    "            body = body.replace(\"\\n\", \"\")\n",
    "            body = body.replace(\"\\\\n\", \"\")\n",
    "            body = body.replace(\",\", \"\")\n",
    "        except:\n",
    "            print(\"error replacing new lines\")\n",
    "        results.append([subreddit, [created_utc, author, body, comment_or_submission]])\n",
    "    return results\n",
    "\n",
    "def getHistoricalData(after_epoch, before_epoch):\n",
    "    final_posts = []\n",
    "    for subreddit in subreddits:\n",
    "        time.sleep(2)\n",
    "        posts1 = urllib.request.urlopen(buildBasicRequest(getBaseCommentURL(), subreddit, str(before_epoch), str(after_epoch), str(200))).read()\n",
    "        posts2 = urllib.request.urlopen(buildBasicRequest(getBaseSubmissionURL(), subreddit, str(before_epoch), str(after_epoch), str(200))).read()\n",
    "        parsed_posts = parsePosts(subreddit, posts1, \"body\", \"comment\")\n",
    "        parsed_posts = parsed_posts + parsePosts(subreddit, posts2, \"title\", \"submission\")\n",
    "        for group in parsed_posts:\n",
    "            final_posts.append([str(group[0]),group[1]])\n",
    "    return final_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + datetime.timedelta(n)\n",
    "\n",
    "\n",
    "# need this starting date for first iteration\n",
    "prior_start_date = datetime.date(2009, 12, 31)\n",
    "\n",
    "# starting at january 1st 2010\n",
    "#start_date = datetime.date(2010, 1, 1)\n",
    "\n",
    "# last day will be day before, so january 1st 2018\n",
    "#end_date = datetime.date(2018, 1, 1)\n",
    "\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    \n",
    "    # get split version - [year, month, day]\n",
    "    prior_date = prior_start_date.strftime(\"%Y-%m-%d\").split('-')\n",
    "    current_date = single_date.strftime(\"%Y-%m-%d\").split('-')\n",
    "    \n",
    "    # get the epochs\n",
    "    after_epoch = getEpochForDate(int(prior_date[0]), int(prior_date[1]), int(prior_date[2]))\n",
    "    before_epoch = getEpochForDate(int(current_date[0]), int(current_date[1]), int(current_date[2]))\n",
    "    \n",
    "    # update before search in case something breaks\n",
    "    prior_start_date = single_date\n",
    "    \n",
    "    result = []\n",
    "    try:\n",
    "        # get that data\n",
    "        result = getHistoricalData(after_epoch, before_epoch)\n",
    "    except:\n",
    "        print('error for current date: ' + str(current_date))\n",
    "\n",
    "    try:\n",
    "        # now we have all of the content for all 5 subreddits for the day, let's get the file name to write to, then write the data\n",
    "        for post_group in result:\n",
    "            file_to_write_data = getFileName(post_group[0], current_date[0])\n",
    "            content = post_group[1]\n",
    "            data_to_write = str(content[0]) + \",\" + str(content[1]) + \",\" + str(content[2]) + \",\" + str(content[3]) + \"\\n\"\n",
    "            f = open(file_to_write_data, \"+a\")\n",
    "            f.write(data_to_write)\n",
    "            f.close()\n",
    "    except:\n",
    "        print(\"error writing results to file:\")\n",
    "        print(result)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
